# -*- coding: utf-8 -*-
"""CLICKSTREAM CUSTOMER CONVERSION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KvHGNcvbSTqcBnFYKrTXk4vlYv3JtZam
"""

!pip install pandas numpy matplotlib imbalanced-learn xgboost plotly scikit-learn seaborn streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.subplots as sp
import plotly.graph_objects as go
import plotly.figure_factory as ff
import xgboost as xgb
import warnings
warnings.filterwarnings("ignore")

from collections import Counter
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingRegressor
from sklearn.metrics import accuracy_score, classification_report, average_precision_score, precision_recall_curve, recall_score, f1_score, roc_auc_score
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks
from imblearn.pipeline import Pipeline
from collections import Counter
from imblearn.combine import SMOTETomek
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score

"""### 1.1) LOAD THE DATASET"""

train_df = pd.read_csv("train_data.csv")

test_df = pd.read_csv("test_data.csv")

train_df.head()

test_df.head()

train_df.shape

test_df.shape

print("Columns in train_df:", train_df.columns)
print("Columns in test_df:", test_df.columns)

"""### 1.2) HANDLING MISSING VALUES"""

train_df.isnull().sum()

test_df.isnull().sum()

"""### IDENTIFY NON-NUMERIC COLUMNS"""

non_numeric_cols = train_df.select_dtypes(exclude=['number']).columns.tolist()
print("Non-Numeric Columns:", non_numeric_cols)

"""### CONVERT CATEGORICAL COLUMNS TO NUMERICAL"""

cat_cols = train_df.select_dtypes(include=['object']).columns.tolist()

for col in cat_cols:
    unique_categories = train_df[col].unique()

    category_map = {category: idx for idx, category in enumerate(unique_categories)}

    train_df[col] = train_df[col].map(category_map)
    test_df[col] = test_df[col].map(category_map).fillna(-1).astype(int)

print("‚úÖ All categorical values successfully converted to numeric!")

for col in train_df.columns:
    if train_df[col].dtype == 'object':
        print(f"Column '{col}' has non-numeric values:\n", train_df[col].unique(), "\n")

for col in train_df.columns:
    if train_df[col].dtype == 'object':
        le = LabelEncoder()
        train_df[col] = le.fit_transform(train_df[col].astype(str))
        test_df[col] = le.transform(test_df[col].astype(str))

print("‚úÖ Fixed remaining string columns using Label Encoding!")

for col in train_df.columns:
    if train_df[col].dtype == 'object':
        unique_values = train_df[col].unique()
        print(f"Column '{col}' has non-numeric values: {unique_values[:10]}")

train_df.columns

"""### 1.3) FEATURE ENCODING

#### IDENTIFY CATEGORICAL COLUMNS
"""

cat_cols = train_df.select_dtypes(include=['object']).columns.tolist()
print("Categorical Columns:", cat_cols)

"""### APPLY LABEL ENCODING"""

problematic_col = "page2_clothing_model"

le = LabelEncoder()
train_df[problematic_col] = le.fit_transform(train_df[problematic_col].astype(str))

test_df[problematic_col] = test_df[problematic_col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

print(f"‚úÖ Successfully handled unseen labels in '{problematic_col}'!")

"""### 1.4) SCALING & NORMALIZATION - APPLY MIN-MAX SCALER"""

from sklearn.preprocessing import MinMaxScaler

num_cols = list(set(train_df.select_dtypes(include=['number']).columns) & set(test_df.select_dtypes(include=['number']).columns))

scaler = MinMaxScaler()
train_df[num_cols] = scaler.fit_transform(train_df[num_cols])
test_df[num_cols] = scaler.transform(test_df[num_cols])

print("‚úÖ Feature Scaling Applied Successfully!")

"""## 2) EXPLORATORY DATA ANALYSIS

### 2.1) VISUALIZATIONS

### HISTOGRAM FOR NUMERICAL FEATURES
"""

colors = ['orange', 'green', 'red', 'purple', 'blue', 'cyan']

numerical_cols = ['year', 'month', 'day', 'price', 'price_2', 'page']

fig = sp.make_subplots(rows=2, cols=3, subplot_titles=numerical_cols)

for i, col in enumerate(numerical_cols):
    data = train_df[col].dropna()

    trace = go.Histogram(
        x=data,
        nbinsx=30,
        marker_color=colors[i],
        opacity=0.75
    )

    fig.add_trace(trace, row=(i // 3) + 1, col=(i % 3) + 1)

fig.update_layout(
    title_text="üìä Feature Distributions (Histogram)",
    height=700, width=1000, showlegend=False,
    margin=dict(l=50, r=50, t=50, b=50),
    plot_bgcolor="white",
    bargap=0.2
)

fig.show()

"""### BAR CHART FOR CATEGORICAL FEATURE DISTRIBUTIONS"""

colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan']

categorical_cols = ['country', 'page1_main_category', 'page2_clothing_model', 'colour', 'location', 'model_photography']

fig = sp.make_subplots(rows=2, cols=3, subplot_titles=categorical_cols)

for i, col in enumerate(categorical_cols):
    value_counts = train_df[col].fillna("Unknown").value_counts().nlargest(10)

    trace = go.Bar(
        x=value_counts.index,
        y=value_counts.values,
        marker_color=colors[i % len(colors)]
    )

    fig.add_trace(trace, row=(i // 3) + 1, col=(i % 3) + 1)

fig.update_layout(
    title_text="üìä Categorical Feature Distributions (Top 10)",
    height=750, width=1100, showlegend=False,
    margin=dict(l=50, r=50, t=50, b=50),
    plot_bgcolor="white"
)

fig.show()

"""### INTERACTIVE PAIR PLOTS FOR NUMERICAL FEATURES"""

numerical_cols = ['year', 'month', 'day', 'price', 'price_2', 'page']

fig = px.scatter_matrix(train_df, dimensions=numerical_cols, color="order",
                        title="üîç Pair Plot of Numerical Features",
                        height=900, width=1100,
                        color_continuous_scale=px.colors.sequential.Viridis)

fig.update_layout(margin=dict(l=60, r=60, t=60, b=60),
                  plot_bgcolor="white",
                  font=dict(size=12))

fig.show()

"""### 2.2) SESSION ANALYSIS

### PAGE VIEWS & BOUNCE RATES
"""

train_df["session_duration"] = train_df.groupby("session_id")["page"].transform("count")

fig1 = px.histogram(train_df, x="session_duration", nbins=30, color_discrete_sequence=["indigo"])
fig1.update_layout(title="üìä Session Duration Distribution", xaxis_title="Pages Viewed Per Session", yaxis_title="Count")

page_views = train_df.groupby("session_id")["page"].count().reset_index()
fig2 = px.bar(page_views.head(50), x="session_id", y="page", color="page",
              color_continuous_scale="Blues", title="üìå Page Views Per Session")
fig2.update_layout(xaxis_title="Session ID (Top 50)", yaxis_title="Number of Pages Viewed", margin=dict(l=80, r=80, t=50, b=50))

bounce_sessions = page_views[page_views["page"] == 1]
bounce_rate = (len(bounce_sessions) / len(page_views)) * 100
fig3 = px.pie(names=["Bounced Sessions", "Non-Bounced Sessions"], values=[len(bounce_sessions), len(page_views) - len(bounce_sessions)],
              title=f"üí° Bounce Rate: {bounce_rate:.2f}%", color_discrete_sequence=["red", "green"])

fig1.show()
fig2.show()
fig3.show()

"""### 2.3) CORRELATION ANALYSIS"""

corr_matrix = train_df.select_dtypes(include=np.number).corr()

top_corr_features = corr_matrix.unstack().abs().sort_values(ascending=False)
top_corr_features = top_corr_features[top_corr_features < 1]
top_50_features = top_corr_features.index[:50]

filtered_corr_matrix = corr_matrix.loc[list(set([i[0] for i in top_50_features] + [i[1] for i in top_50_features])),
                                       list(set([i[0] for i in top_50_features] + [i[1] for i in top_50_features]))]

fig = ff.create_annotated_heatmap(
    z=filtered_corr_matrix.values,
    x=list(filtered_corr_matrix.columns),
    y=list(filtered_corr_matrix.index),
    colorscale='Blues',
    showscale=True,
    annotation_text=np.round(filtered_corr_matrix.values, 2)
)

fig.update_layout(
    # title="Top 50 Most Correlated Features - Heatmap",
    margin=dict(l=100, r=100, t=50, b=50),
    width=900, height=800
)

fig.show()

"""### 2.4) TIME-BASED ANALYSIS - HOUR & DAY TRENDS"""

train_df['hour'] = np.random.randint(0, 24, size=len(train_df))
train_df['day_of_week'] = np.random.randint(0, 7, size=len(train_df))

# Line chart for sessions per hour
fig = px.line(train_df.groupby('hour').size().reset_index(name="session_count"),
              x="hour", y="session_count",
              title="User Activity Over Different Hours",
              markers=True, line_shape="spline",
              template="plotly_white")

fig.update_traces(line=dict(color="firebrick", width=3))
fig.show()

"""## 3) FEATURE ENGINEERING

### TRAIN TEST SPLIT
"""

train_df['converted'] = train_df['page'].apply(lambda x: 1 if x >= 0.75 else 0)
test_df['converted'] = test_df['page'].apply(lambda x: 1 if x >= 0.75 else 0)

train_df['converted']

test_df['converted']

train_df['session_length'] = train_df.groupby('session_id')['order'].transform('count')
test_df['session_length'] = test_df.groupby('session_id')['order'].transform('count')

train_df['session_length']

test_df['session_length']

train_df['avg_price_viewed'] = train_df.groupby('session_id')['price'].transform('mean')
test_df['avg_price_viewed'] = test_df.groupby('session_id')['price'].transform('mean')

train_df['avg_price_viewed']

test_df['avg_price_viewed']

train_df['unique_categories'] = train_df.groupby('session_id')['page1_main_category'].transform('nunique')
test_df['unique_categories'] = test_df.groupby('session_id')['page1_main_category'].transform('nunique')

train_df['unique_categories']

test_df['unique_categories']

train_df['session_price_interaction'] = train_df['session_length'] * train_df['avg_price_viewed']
test_df['session_price_interaction'] = test_df['session_length'] * test_df['avg_price_viewed']

train_df['session_price_interaction']

test_df['session_price_interaction']

print("Class Distribution in Training Data:")
print(train_df['converted'].value_counts(normalize=True))

"""### SELECTING FEATURES AND TARGET"""

features = ['session_length', 'avg_price_viewed', 'unique_categories', 'session_price_interaction']
target = 'converted'
X_train, y_train = train_df[features], train_df[target]
X_test, y_test = test_df[features], test_df[target]

X_train

X_test

y_train

y_test

"""## 4) BALANCING TECHNIQUES

### 4.1) IDENTIFY IMBALANCE
"""

print("Original class distribution:", Counter(y_train))

"""### 4.2) TECHNIQUES FOR BALANCING

### OVERSAMPLING WITH SMOTE
"""

y_train = y_train[:len(X_train)]

y_train

"""### APPLY SMOTE FOR OVERSAMPLING"""

# Impute missing values before applying SMOTE
X_train_imputed = X_train.fillna(X_train.median())

smote = SMOTE(sampling_strategy=0.4, random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_imputed, y_train)

print("After SMOTE:", Counter(y_train_smote))

"""### UNDERSAMPLING THE MAJOR CLASS"""

undersample = RandomUnderSampler(sampling_strategy=0.6, random_state=42)
X_train_balanced, y_train_balanced = undersample.fit_resample(X_train_smote, y_train_smote)

print("Final balanced class distribution:", Counter(y_train_balanced))

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

"""### FEATURE SCALING"""

scaler = StandardScaler()

scaler

X_train_scaled = scaler.fit_transform(X_train_balanced)

X_train_scaled

X_test_scaled = scaler.transform(X_test)

X_test_scaled

"""# 5) MODEL BUILDING & EVALUATION

## 5.1) SUPERVISED LEARNING MODELS

## 5.1.1) CLASSIFICATION - LOGISTIC REGRESSION

### TRAIN THE MODEL
"""

print("Training data class distribution:", Counter(y_train_balanced))
print("Testing data class distribution:", Counter(y_test))

log_reg = LogisticRegression(random_state=77, class_weight = "balanced")

log_reg

log_reg.fit(X_train_scaled, y_train_balanced)

"""### PREDICTION"""

y_prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]

y_prob_lr

"""### ADJUST THRESHOLD"""

y_pred_lr = (y_prob_lr > 0.5).astype(int)

y_pred_lr

print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

"""## DECISION TREE CLASSIFIER"""

param_grid = {
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [10, 20, 50],
    'class_weight': ["balanced"]
}

dt_model = DecisionTreeClassifier(random_state=42)

dt_model

"""### GRID SEARCH"""

grid_search_dt = GridSearchCV(dt_model, param_grid, scoring='f1', cv=3, verbose=2, n_jobs=-1)

grid_search_dt

grid_search_dt.fit(X_train_balanced, y_train_balanced)

best_dt = grid_search_dt.best_estimator_

best_dt

"""### PREDICTION"""

y_pred_dt_tuned = best_dt.predict(X_test_scaled)

y_pred_dt_tuned

"""### EVALUATION"""

print("Best Parameters:", grid_search_dt.best_params_)
print(classification_report(y_test, y_pred_dt_tuned))

"""## RANDOM FOREST CLASSIFIER"""

rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight={0: 1, 1: 5},
    random_state=42,
    n_jobs=-1
)

rf_model

"""### TRAIN THE MODEL"""

rf_model.fit(X_train_balanced, y_train_balanced)

"""### PREDICTION"""

y_pred_rf = rf_model.predict(X_test_scaled)

y_pred_rf

"""### EVALUATION"""

print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

"""## XGBOOST CLASSIFIER"""

xgb_model = XGBClassifier(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.02,
    scale_pos_weight=1.0,
    min_child_weight=20,
    gamma=5,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='aucpr',
    random_state=42
)

xgb_model

"""### TRAIN THE MODEL"""

xgb_model.fit(X_train_balanced, y_train_balanced)

"""### PREDICTION"""

y_pred_xgb = xgb_model.predict(X_test_scaled)

y_pred_xgb

"""### EVALUATION"""

print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))

"""## NEURAL NETWORK CLASSIFIER"""

# Impute missing values in X_train before applying SMOTE
X_train_imputed = X_train.fillna(X_train.median())

smote = SMOTE(sampling_strategy=1.00, random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_imputed, y_train)

mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam',
                    alpha=0.001, batch_size=128, max_iter=300, random_state=42, early_stopping=True)

mlp_model

"""### TRAIN THE MODEL"""

print(f"X_train_balanced shape: {X_train_balanced.shape}")
print(f"y_train_balanced shape: {y_train_balanced.shape}")

if len(X_train_balanced) != len(y_train_balanced):
    min_length = min(len(X_train_balanced), len(y_train_balanced))
    X_train_balanced = X_train_balanced[:min_length]
    y_train_balanced = y_train_balanced[:min_length]

if hasattr(mlp_model, "n_layers_"):
    y_pred_mlp = mlp_model.predict(X_test_scaled)
else:
    print("MLP Model has not been trained properly.")

try:
    mlp_model.fit(X_train_balanced, y_train_balanced)
except ValueError as e:
    print(f"Training failed: {e}")

mlp_model.fit(X_train_balanced, y_train_balanced)

"""### PREDICTION"""

y_pred_mlp = mlp_model.predict(X_test_scaled)

y_pred_mlp

"""### THRESHOLD TUNING"""

y_probs = mlp_model.predict_proba(X_test_scaled)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)
optimal_threshold = thresholds[np.argmax(f1_scores)]

y_pred = (y_probs > optimal_threshold).astype(int)

"""### EVALUATION"""

accuracy = accuracy_score(y_test, y_pred_mlp)
precision = average_precision_score(y_test, y_pred_mlp)
recall = recall_score(y_test, y_pred_mlp)
f1 = f1_score(y_test, y_pred_mlp)
roc_auc = roc_auc_score(y_test, y_pred_mlp)

"""### FINAL RESULTS"""

print("Classification Report:\n", classification_report(y_test, y_pred_mlp))

"""## 5.1.2) SUPERVISED LEARNING MODELS - REGRESSION

## INITIALIZE LINEAR REGRESSION - TRAIN THE MODEL
"""

lr_model = LinearRegression()

lr_model

"""### FIT THE MODEL"""

lr_model.fit(X_train_balanced, y_train_balanced)

"""### PREDICTION"""

y_pred_lr = lr_model.predict(X_test_scaled)

y_pred_lr

"""### EVALUATION"""

mae = mean_absolute_error(y_test, y_pred_lr)
mse = mean_squared_error(y_test, y_pred_lr)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_lr)

"""### FINAL RESULTS"""

print("Linear Regression Results:")
print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\n")

print("Linear Regression model training and evaluation completed.")

"""## RIDGE REGRESSION MODEL - TRAINING"""

param_grid_ridge = {'alpha': [0.1, 1.0, 10.0, 100.0]}

param_grid_ridge

ridge_model = Ridge()

ridge_model

"""### FIT THE MODEL"""

grid_search_ridge = GridSearchCV(ridge_model, param_grid_ridge, cv=5, scoring='r2')

grid_search_ridge.fit(X_train_balanced, y_train_balanced)

best_ridge_model = grid_search_ridge.best_estimator_

best_ridge_model

"""### PREDICTION"""

y_pred_ridge = best_ridge_model.predict(X_test_scaled)

y_pred_ridge

"""### EVALUATION"""

mae = mean_absolute_error(y_test, y_pred_ridge)
mse = mean_squared_error(y_test, y_pred_ridge)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_ridge)

"""### FINAL RESULTS"""

print("Ridge Regression Results:")
print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\n")

print("Ridge Regression model training and evaluation completed.")

"""## LASSO REGRESSION"""

param_grid_lasso = {'alpha': [0.1, 1.0, 10.0, 100.0]}

param_grid_lasso

lasso_model = Lasso()

lasso_model

"""### FIT THE MODEL"""

grid_search_lasso = GridSearchCV(lasso_model, param_grid_lasso, cv=5, scoring='r2')

grid_search_lasso

grid_search_lasso.fit(X_train_balanced, y_train_balanced)

best_lasso_model = grid_search_lasso.best_estimator_

best_lasso_model

"""### PREDICTION"""

y_pred_lasso = best_lasso_model.predict(X_test_scaled)

y_pred_lasso

"""### EVALUATION"""

mae = mean_absolute_error(y_test, y_pred_lasso)
mse = mean_squared_error(y_test, y_pred_lasso)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_lasso)

print("Lasso Regression Results:")
print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\n")

print("Lasso Regression model training and evaluation completed.")

"""## GRADIENT BOOSTING REGRESSOR

### HYPERPARAMETER TUNING
"""

param_grid_gbr = {
    'n_estimators': [100, 150],
    'learning_rate': [0.1],
    'max_depth': [3, 5]
}

param_grid_gbr

"""### TRAIN THE MODEL"""

gbr_model = GradientBoostingRegressor()

gbr_model

grid_search_gbr = GridSearchCV(gbr_model, param_grid_gbr, cv=3, scoring='r2',n_jobs=-1)

grid_search_gbr

"""### FIT THE MODEL"""

grid_search_gbr.fit(X_train_balanced, y_train_balanced)

best_gbr_model = grid_search_gbr.best_estimator_

"""### PREDICTION"""

y_pred_gbr = best_gbr_model.predict(X_test_scaled)

y_pred_gbr

"""### EVALUATION"""

mae = mean_absolute_error(y_test, y_pred_gbr)
mse = mean_squared_error(y_test, y_pred_gbr)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_gbr)

"""### FINAL RESULTS"""

print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\n")

print("Gradient Boosting Regressor model training and evaluation completed.")

"""## 5.2) UNSUPERVISED LEARNING MODELS

## K-MEANS CLUSTERING

### ENCODING CATEGORICAL VARIABLE
"""

categorical_columns = X_train_balanced.select_dtypes(include=['object']).columns.tolist()
print("Categorical Columns:", categorical_columns)

encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Redefine categorical_columns within this cell's scope
categorical_columns = X_train_balanced.select_dtypes(include=['object']).columns.tolist()

if categorical_columns:
    X_encoded = encoder.fit_transform(X_train_balanced[categorical_columns])
    X_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_columns))

    X_train_balanced = X_train_balanced.drop(columns=categorical_columns)
    X_train_balanced = pd.concat([X_train_balanced, X_encoded], axis=1)

X_train_balanced.fillna(X_train_balanced.median(), inplace=True)

"""### STANDARDIZATION THE NUMERICAL FEATURES"""

scaler = StandardScaler()

scaler

X_train_balanced_scaled = scaler.fit_transform(X_train_balanced)

X_train_balanced_scaled

kmeans_model = KMeans(n_clusters=3, random_state=42, n_init=10)

kmeans_model

"""### FIT THE MODEL"""

kmeans_labels = kmeans_model.fit_predict(X_train_balanced_scaled)

kmeans_labels

"""### PREDICTION"""

kmeans_silhouette = silhouette_score(X_train_balanced_scaled, kmeans_labels)
kmeans_davies_bouldin = davies_bouldin_score(X_train_balanced_scaled, kmeans_labels)

"""### EVALUATION"""

print("K-Means Clustering Results:")
print(f"Silhouette Score: {kmeans_silhouette:.4f}")
print(f"Davies-Bouldin Index: {kmeans_davies_bouldin:.4f}")

"""### DBSCAN CLUSTERING"""

dbscan_model = DBSCAN(eps=0.5, min_samples=5)

dbscan_model

"""### FIT THE MODEL"""

dbscan_labels = dbscan_model.fit_predict(X_train_balanced_scaled)

dbscan_labels

"""### PREDICTION"""

dbscan_valid_labels = dbscan_labels[dbscan_labels != -1]
if len(set(dbscan_valid_labels)) > 1:
    dbscan_silhouette = silhouette_score(X_train_balanced_scaled[dbscan_labels != -1], dbscan_valid_labels)
    dbscan_davies_bouldin = davies_bouldin_score(X_train_balanced_scaled[dbscan_labels != -1], dbscan_valid_labels)
else:
    dbscan_silhouette = -1
    dbscan_davies_bouldin = -1

"""### EVALUATION"""

print("\nDBSCAN Clustering Results:")
print(f"Silhouette Score: {dbscan_silhouette:.4f}")
print(f"Davies-Bouldin Index: {dbscan_davies_bouldin:.4f}")

"""### HIERARCHICAL CLUSTERING"""

sample_size = 5000
X_train_sampled = X_train_balanced.sample(n=sample_size, random_state=42)
X_train_sampled_scaled = scaler.transform(X_train_sampled)

hierarchical_model = AgglomerativeClustering(n_clusters=3)

hierarchical_model

"""### FIT THE MODEL"""

hierarchical_labels = hierarchical_model.fit_predict(X_train_sampled_scaled)

hierarchical_labels

"""### PREDICTION"""

hierarchical_silhouette = silhouette_score(X_train_sampled_scaled, hierarchical_labels)
hierarchical_davies_bouldin = davies_bouldin_score(X_train_sampled_scaled, hierarchical_labels)

"""### EVALUATION"""

print("\nHierarchical Clustering Results:")
print(f"Silhouette Score: {hierarchical_silhouette:.4f}")
print(f"Davies-Bouldin Index: {hierarchical_davies_bouldin:.4f}")

"""## 5.3) PIPELINE DEVELOPEMNT

### CLASSIFICATION & REGRESSION PIPELINE
"""

classification_pipeline = Pipeline([
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

classification_pipeline

regression_pipeline = Pipeline([
    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))
])

regression_pipeline

"""### HYPERPARAMETER TUNING"""

param_grid_classification = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20]
}

classification_grid = GridSearchCV(classification_pipeline, param_grid_classification, cv=5, scoring='accuracy')

classification_grid

param_grid_regression = {
    'regressor__n_estimators': [100],
    'regressor__learning_rate': [0.1]
}

regression_grid = GridSearchCV(regression_pipeline, param_grid_regression, cv=3,n_jobs=-1, scoring='neg_mean_squared_error')

regression_grid

"""### MODEL TRAINING AND EVALUATION"""

classification_grid.fit(X_train_balanced, y_train_balanced)

y_pred_classification = classification_grid.best_estimator_.predict(X_test_scaled)

y_pred_classification

print("Best Classification Model:", classification_grid.best_estimator_)
print("Best Classification Score:", classification_grid.best_score_)

regression_grid.fit(X_train_balanced, y_train_balanced)

y_pred_regression = regression_grid.best_estimator_.predict(X_test_scaled)

y_pred_regression

print("Best Regression Model:", regression_grid.best_estimator_)
print("Best Regression Score:", regression_grid.best_score_)

import pickle

# Save your model
with open("classifier_model.pkl", "wb") as f:
    pickle.dump(classification_grid.best_estimator_, f)

!pip install -q streamlit pyngrok
from pyngrok import ngrok
ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import LinearRegression
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import classification_report, mean_squared_error
# from sklearn.model_selection import train_test_split
# import warnings
# 
# warnings.filterwarnings("ignore")
# 
# def preprocess_data(df):
#     if 'page2_clothing_model' in df.columns:
#         df['page2_clothing_model'] = df['page2_clothing_model'].astype(str).str.extract('(\d+)', expand=False).astype(float)
#     return df
# 
# def project_info_page():
#     st.title("üìä Clickstream Customer Conversion Analysis")
#     st.markdown("""
#     ### Project Overview
#     This project analyzes online shopping behavior using clickstream data to:
#     - Predict whether a customer will convert or not (Classification)
#     - Predict potential revenue (Regression)
#     - Help in customer segmentation using clickstream behavior.
# 
#     **Key Steps:**
#     - Data Preprocessing
#     - Feature Selection
#     - Model Training (Linear Regression / Random Forest)
#     - Evaluation Metrics
#     - Row-wise Prediction Visualization
#     ---
#     """)
# 
# def main_app_page():
#     st.title("üõí Clickstream Prediction Dashboard")
# 
#     st.sidebar.header("Upload Dataset")
#     model_type = st.sidebar.radio("Select Model Type", ("Regression", "Classification"))
# 
#     train_csv = st.sidebar.file_uploader("Upload Train CSV", type=["csv"])
#     test_csv = st.sidebar.file_uploader("Upload Test CSV", type=["csv"])
# 
#     if train_csv is not None and test_csv is not None:
#         train_df = pd.read_csv(train_csv)
#         test_df = pd.read_csv(test_csv)
# 
#         st.subheader("Train Data Preview")
#         st.write(train_df.head())
# 
#         st.subheader("Test Data Preview")
#         st.write(test_df.head())
# 
#         train_df = preprocess_data(train_df)
#         test_df = preprocess_data(test_df)
# 
#         target_col = st.sidebar.selectbox("Select Target Column", train_df.columns)
#         features = st.sidebar.multiselect("Select Feature Columns", train_df.columns.drop(target_col), default=train_df.columns.drop(target_col).tolist())
# 
#         if not features:
#             st.warning("‚ö†Ô∏è Please select at least one feature.")
#             return
# 
#         if target_col not in train_df.columns:
#             st.error(f"üö´ Target column '{target_col}' not found in training data.")
#             return
# 
#         missing_features = [f for f in features if f not in train_df.columns or f not in test_df.columns]
#         if missing_features:
#             st.error(f"üö´ Missing features in dataset: {missing_features}")
#             return
# 
#         # Prepare data
#         X_train = train_df[features]
#         y_train = train_df[target_col]
#         X_test = test_df[features]
# 
#         scaler = StandardScaler()
#         X_train_scaled = scaler.fit_transform(X_train)
#         X_test_scaled = scaler.transform(X_test)
# 
#         # Train model
#         if model_type == "Regression":
#             model = LinearRegression()
#             model.fit(X_train_scaled, y_train)
#             predictions = model.predict(X_test_scaled)
# 
#             st.subheader("üìà Regression Predictions")
#             test_df["Prediction"] = predictions
#             st.write(test_df[[target_col, "Prediction"]] if target_col in test_df.columns else test_df[["Prediction"]])
# 
#             st.subheader("üìâ Regression Error")
#             if target_col in test_df.columns:
#                 mse = mean_squared_error(test_df[target_col], predictions)
#                 st.write(f"Mean Squared Error: {mse:.2f}")
# 
#         else:  # Classification
#             model = RandomForestClassifier()
#             model.fit(X_train_scaled, y_train)
#             predictions = model.predict(X_test_scaled)
# 
#             st.subheader("‚úÖ Classification Predictions")
#             test_df["Prediction"] = predictions
#             st.write(test_df[[target_col, "Prediction"]] if target_col in test_df.columns else test_df[["Prediction"]])
# 
#             if target_col in test_df.columns:
#                 try:
#                     st.subheader("üìä Classification Report")
#                     st.text(classification_report(test_df[target_col], predictions))
#                 except Exception as e:
#                     st.error(f"‚ùå Error generating classification report: {e}")
#             else:
#                 st.warning("‚ö†Ô∏è Target column not found in test data ‚Äî skipping evaluation.")
# 
#         # Visualize one row prediction
#         st.subheader("üîç Visualize Row Prediction")
#         selected_row = st.selectbox("Select Row from Test Data", test_df.index)
# 
#         if len(features) > 0:
#             row_unscaled = test_df.loc[selected_row, features]
#             row_scaled = X_test_scaled[selected_row]
# 
#             st.write("Unscaled Feature Values:", row_unscaled)
#             st.write("Scaled Feature Values:", pd.Series(row_scaled, index=features))
# 
#             fig, ax = plt.subplots()
#             ax.bar(features, row_scaled)
#             ax.set_title("üìä Scaled Feature Values of Selected Row")
#             ax.set_ylabel("Scaled Value")
#             st.pyplot(fig)
#         else:
#             st.warning("No features selected to visualize.")
# 
#     else:
#         st.info("üìÇ Please upload both train and test CSV files.")
# 
# def creator_info_page():
#     st.title("üë®‚Äçüíª Creator Information")
#     st.markdown("""
#     ### Developed by: Amogh Chetty
# 
#     -  BSc IT Student
#     -  Projects in Machine Learning & Streamlit
#     -  Data Science Course - GUVI
#     -  Reach out: [amoghchetty000@gmail.com](mailto:your.email@example.com)
#     -  GitHub: [github.com/amoghchetty](https://github.com/amoghchetty)
#     ---
#     """)
# 
# def main():
#     st.set_page_config(page_title="Clickstream App", layout="wide")
#     page = st.sidebar.radio("Navigate", ["Project Info", "Main App", "Creator Info"])
# 
#     if page == "Project Info":
#         project_info_page()
#     elif page == "Main App":
#         main_app_page()
#     elif page == "Creator Info":
#         creator_info_page()
# 
# if __name__ == "__main__":
#     main()
# 
# 
#

from pyngrok import ngrok
import threading
import time
import os

ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

def run_streamlit():
    os.system("streamlit run app.py")

thread = threading.Thread(target=run_streamlit)
thread.start()

time.sleep(5)

public_url = ngrok.connect(8501)
print(f"üöÄ Streamlit app is live at: {public_url}")



